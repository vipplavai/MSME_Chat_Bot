{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2205749e49d44ce093a0f3e9a92fc962": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85e30cb3c2d94bdb9376c71263439f95",
              "IPY_MODEL_4146ee61a6f34d1e9cb7f644af7e2cba",
              "IPY_MODEL_bb9764fdb22445c8b3b84630dde86204"
            ],
            "layout": "IPY_MODEL_e8ef9dc76b0544f181c318f202a1dcfc"
          }
        },
        "85e30cb3c2d94bdb9376c71263439f95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9c01063e63543db9d5f00e483ab9295",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1ab1f976b5984d479cf11d6d8569b4d5",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "4146ee61a6f34d1e9cb7f644af7e2cba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ed0414a799643279a37999eb6fd6a8d",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_711441a077cd4cacaa347d9b7d83b28f",
            "value": 2
          }
        },
        "bb9764fdb22445c8b3b84630dde86204": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3e17d9b793040619d5bdf317d81a5d5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_30780d4d88bf42199b17cc116d7b010b",
            "value": "‚Äá2/2‚Äá[00:01&lt;00:00,‚Äá‚Äá1.44s/it]"
          }
        },
        "e8ef9dc76b0544f181c318f202a1dcfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9c01063e63543db9d5f00e483ab9295": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ab1f976b5984d479cf11d6d8569b4d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ed0414a799643279a37999eb6fd6a8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "711441a077cd4cacaa347d9b7d83b28f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c3e17d9b793040619d5bdf317d81a5d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30780d4d88bf42199b17cc116d7b010b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "12fYxc0NbBui"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/AI4Bharat/IndicTrans2.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%cd /content/IndicTrans2/huggingface_interface"
      ],
      "metadata": {
        "id": "5hqeIiHTbNpT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!python3 -m pip install nltk sacremoses pandas regex mock transformers>=4.33.2 mosestokenizer\n",
        "!python3 -c \"import nltk; nltk.download('punkt')\"\n",
        "!python3 -m pip install bitsandbytes scipy accelerate datasets\n",
        "!python3 -m pip install sentencepiece\n",
        "\n",
        "!git clone https://github.com/VarunGumma/IndicTransToolkit.git\n",
        "%cd IndicTransToolkit\n",
        "!python3 -m pip install --editable ./\n",
        "%cd .."
      ],
      "metadata": {
        "id": "HlJqLQRwbP2g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymongo sentence-transformers torch transformers langchain_community pymupdf tools --quiet\n"
      ],
      "metadata": {
        "id": "P1WzuS59bR-M"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**After Intsalling above packages, restart the run time and run the below code **"
      ],
      "metadata": {
        "id": "2w-PO2hwtR9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PDF Upload"
      ],
      "metadata": {
        "id": "tdkRnybJjw7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet pymupdf\n",
        "\n",
        "import pymupdf\n",
        "import torch\n",
        "from pymongo import MongoClient\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from google.colab import files\n",
        "\n",
        "# === MongoDB ===\n",
        "mongo_uri = \"mongodb+srv://vipplavai:pravip2025@cluster0.zcsijsa.mongodb.net/\"\n",
        "client = MongoClient(mongo_uri)\n",
        "temp_coll = client[\"msme_schemes_db\"][\"uploaded_pdf_temp\"]\n",
        "\n",
        "# === Embedding Model ===\n",
        "embed_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# === Upload PDF ===\n",
        "uploaded = files.upload()\n",
        "pdf_path = next(iter(uploaded))\n",
        "\n",
        "# === Chunk Function ===\n",
        "def chunk_text(text, chunk_size=350, overlap=50):\n",
        "    tokens = text.split()\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        chunks.append(\" \".join(tokens[i:i+chunk_size]))\n",
        "        i += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "# === Process PDF ===\n",
        "try:\n",
        "    doc = pymupdf.open(pdf_path)\n",
        "    full_text = \"\\n\".join([page.get_text().strip() for page in doc])\n",
        "    if not full_text:\n",
        "        print(\"‚ùå No extractable text found. PDF might be scanned.\")\n",
        "    else:\n",
        "        chunks = chunk_text(full_text)\n",
        "        doc_chunks = [{\"chunk_id\": i, \"chunk_text\": c, \"embedding\": embed_model.encode(c).tolist()} for i, c in enumerate(chunks)]\n",
        "        temp_coll.delete_many({})\n",
        "        temp_coll.insert_one({\"source\": \"user_uploaded\", \"rag_chunks\": doc_chunks})\n",
        "        print(f\"‚úÖ Stored {len(doc_chunks)} chunks in MongoDB.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå PDF processing failed: {e}\")\n"
      ],
      "metadata": {
        "id": "RaxAlJq6nlUC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "faef0482-665a-42a0-b370-9fac16dc260f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bc6f4862-0458-4190-acec-46936b1a5706\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bc6f4862-0458-4190-acec-46936b1a5706\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving T-PRIDE Scheme.pdf to T-PRIDE Scheme (2).pdf\n",
            "‚úÖ Stored 93 chunks in MongoDB.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch, re\n",
        "from pymongo import MongoClient\n",
        "from datetime import datetime\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM, pipeline,\n",
        "    AutoModelForSeq2SeqLM\n",
        ")\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from IndicTransToolkit.processor import IndicProcessor\n",
        "from peft import PeftModel\n",
        "\n",
        "# === MongoDB Setup ===\n",
        "mongo_uri = \"mongodb+srv://vipplavai:pravip2025@cluster0.zcsijsa.mongodb.net/\"\n",
        "client = MongoClient(mongo_uri)\n",
        "db = client[\"msme_schemes_db\"]\n",
        "udyam_coll = db[\"udyam_profiles\"]\n",
        "schemes_chunk_coll = db[\"schemes_chunks_only\"]\n",
        "schemes_info_coll = db[\"schemes_embedded\"]\n",
        "query_logs_coll = db[\"query_logs\"]\n",
        "temp_coll = db[\"uploaded_pdf_temp\"]\n",
        "\n",
        "# === LLM + Embeddings ===\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Vipplav/gemma-finetuned-faq\", use_fast=True)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"google/gemma-2b-it\", device_map=\"auto\",\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ")\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model, \"Vipplav/gemma-finetuned-faq\", device_map=\"auto\",\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ")\n",
        "generator = pipeline(\n",
        "    \"text-generation\", model=model, tokenizer=tokenizer,\n",
        "    max_new_tokens=150, do_sample=False\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline=generator)\n",
        "embed_model = SentenceTransformer(\n",
        "    \"BAAI/bge-small-en-v1.5\",\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# === IndicTrans2 Setup ===\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "ip = IndicProcessor(inference=True)\n",
        "translator_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"ai4bharat/indictrans2-en-indic-1B\", trust_remote_code=True\n",
        ")\n",
        "translator_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    \"ai4bharat/indictrans2-en-indic-1B\", trust_remote_code=True\n",
        ").to(DEVICE).eval()\n",
        "\n",
        "def translate_to_telugu(text):\n",
        "    if not text.strip(): return \"‚ö†Ô∏è Nothing to translate.\"\n",
        "    batch = ip.preprocess_batch([text], src_lang=\"eng_Latn\", tgt_lang=\"tel_Telu\")\n",
        "    inputs = translator_tokenizer(batch, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        outputs = translator_model.generate(**inputs, max_length=256, num_beams=5)\n",
        "    decoded = translator_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    return ip.postprocess_batch(decoded, lang=\"tel_Telu\")[0]\n",
        "\n",
        "# === Prompt Template ===\n",
        "rephrase_template = PromptTemplate.from_template(\"\"\"\n",
        "You're a helpful assistant guiding Indian MSMEs to the best-matching government schemes.\n",
        "Based on the enterprise profile, generate a clear, short one-line search query with keywords like state, sector, size, gender, and investment.\n",
        "Only return the query. Avoid comments.\n",
        "Enterprise Profile:\n",
        "{profile_summary}\n",
        "\"\"\")\n",
        "\n",
        "# === MSME Utilities ===\n",
        "def normalize_udyam(uid): return uid.strip().upper().replace(\" \", \"\")\n",
        "def is_valid_udyam(uid): return bool(re.match(r\"^UDYAM-[A-Z]{2}-\\d{2}-\\d{6,7}$\", uid))\n",
        "def get_profile_by_uid(uid):\n",
        "    uid = normalize_udyam(uid)\n",
        "    return udyam_coll.find_one({\"Udyam_ID\": uid}, {\"_id\": 0}) if is_valid_udyam(uid) else None\n",
        "\n",
        "def summarize_profile(p):\n",
        "    return (\n",
        "        f\"The user represents an enterprise named '{p['Enterprise Name']}', based in {p['State']}, \"\n",
        "        f\"operating in the {p['Major Activity']} sector. They identify as {p['Gender']}, run a \"\n",
        "        f\"{p['Enterprise Type']} sized {p['Organisation Type'].lower()} organization. The enterprise has \"\n",
        "        f\"{p['Employment']} employees, with an investment of ‚Çπ{p['Investment Cost (In Rs.)']:,} and a turnover \"\n",
        "        f\"of ‚Çπ{p['Net Turnover (In Rs.)']:,}.\"\n",
        "    )\n",
        "\n",
        "def generate_search_query(profile):\n",
        "    summary = summarize_profile(profile)\n",
        "    q = llm.invoke(rephrase_template.format(profile_summary=summary)).strip().split(\"\\n\")[0].strip()\n",
        "    return q, summary\n",
        "\n",
        "def get_top_matching_schemes(q, top_k=5):\n",
        "    qe = embed_model.encode(q, convert_to_tensor=True)\n",
        "    scores = []\n",
        "    for doc in schemes_chunk_coll.find({\"rag_chunks\": {\"$exists\": True}}):\n",
        "        for chunk in doc[\"rag_chunks\"]:\n",
        "            if \"embedding\" in chunk:\n",
        "                ce = torch.tensor(chunk[\"embedding\"]).to(qe.device)\n",
        "                score = util.cos_sim(qe, ce)[0][0].item()\n",
        "                scores.append((score, doc[\"scheme_id\"], doc[\"scheme_name\"]))\n",
        "    seen, out = set(), []\n",
        "    for score, sid, name in sorted(scores, key=lambda x: x[0], reverse=True):\n",
        "        if sid not in seen:\n",
        "            out.append({\"score\": score, \"scheme_id\": sid, \"scheme_name\": name})\n",
        "            seen.add(sid)\n",
        "        if len(out) == top_k:\n",
        "            break\n",
        "    return out\n",
        "\n",
        "def fetch_scheme_field_llm(scheme_id, query):\n",
        "    fmap = {\n",
        "        \"eligibility\": \"eligibility_list\",\n",
        "        \"benefits\": \"key_benefits_list\",\n",
        "        \"assistance\": \"assistance_list\",\n",
        "        \"apply\": \"how_to_apply_list\",\n",
        "        \"documents\": \"required_documents_list\"\n",
        "    }\n",
        "    key = next((v for k,v in fmap.items() if k in query.lower()), None)\n",
        "    doc = schemes_info_coll.find_one({\"scheme_id\": scheme_id})\n",
        "    if key and doc and key in doc:\n",
        "        text = \"\\n\".join(doc[key][:5])\n",
        "        p = (\n",
        "            f\"Summarize for business owners:\\nScheme: {doc['scheme_name']}\\n\"\n",
        "            f\"Section: {key.replace('_list','').title()}\\n\\n{text}\"\n",
        "        )\n",
        "        return llm.invoke(p).strip()\n",
        "    return \"‚ùå Ask eligibility, benefits, how to apply, or documents.\"\n",
        "\n",
        "# === State ===\n",
        "chat_state = {\"stage\":0,\"profile\":{}, \"scheme_id\":None,\"last_bot_msg\":\"\", \"summary\":\"\"}\n",
        "pdf_state  = {\"last_pdf_msg\":\"\"}\n",
        "\n",
        "# === MSME Chatbot ===\n",
        "# === MSME Chatbot Logic (store last_bot_msg for all replies) ===\n",
        "def chatbot(msg, history):\n",
        "    # decide reply based on stage\n",
        "    if chat_state[\"stage\"] == 0:\n",
        "        response = \"üëã Enter Udyam ID or type 'manual'.\"\n",
        "        chat_state[\"stage\"] = 1\n",
        "\n",
        "    elif chat_state[\"stage\"] == 1:\n",
        "        if msg.lower().startswith(\"udyam-\"):\n",
        "            profile = get_profile_by_uid(msg)\n",
        "            if profile:\n",
        "                summary = summarize_profile(profile)\n",
        "                response = f\"‚úÖ Profile loaded:\\n{summary}\\nType 'show related schemes'.\"\n",
        "                chat_state.update({\"profile\": profile, \"stage\": 3, \"summary\": summary})\n",
        "            else:\n",
        "                response = \"‚ùå Invalid Udyam ID. Try again or type 'manual'.\"\n",
        "        elif \"manual\" in msg.lower():\n",
        "            response = \"üìù What's your enterprise name?\"\n",
        "            chat_state[\"stage\"] = 2\n",
        "        else:\n",
        "            response = \"Please enter a valid Udyam ID or 'manual'.\"\n",
        "\n",
        "    elif chat_state[\"stage\"] == 2:\n",
        "        fields = [\n",
        "            \"Enterprise Name\",\"Gender\",\"Enterprise Type\",\"Organisation Type\",\n",
        "            \"Major Activity\",\"State\",\"Investment Cost (In Rs.)\",\n",
        "            \"Net Turnover (In Rs.)\",\"Employment\"\n",
        "        ]\n",
        "        idx = len(chat_state[\"profile\"])\n",
        "        key = fields[idx]\n",
        "        chat_state[\"profile\"][key] = int(msg) if any(x in key for x in [\"Cost\",\"Turnover\",\"Employment\"]) else msg\n",
        "        if len(chat_state[\"profile\"]) == len(fields):\n",
        "            summary = summarize_profile(chat_state[\"profile\"])\n",
        "            response = f\"‚úÖ Profile saved:\\n{summary}\\nType 'show related schemes'.\"\n",
        "            chat_state.update({\"stage\": 3, \"summary\": summary})\n",
        "        else:\n",
        "            response = f\"{fields[idx+1]}?\"\n",
        "\n",
        "    elif chat_state[\"stage\"] == 3 and \"scheme\" in msg.lower():\n",
        "        query, _ = generate_search_query(chat_state[\"profile\"])\n",
        "        results = get_top_matching_schemes(query)\n",
        "        if not results:\n",
        "            response = \"‚ö†Ô∏è No schemes matched.\"\n",
        "        else:\n",
        "            response = \"üìà Recommended Schemes:\\n\" + \"\\n\".join(\n",
        "                f\"{i+1}. {r['scheme_name']} (Score: {round(r['score'],4)})\"\n",
        "                for i, r in enumerate(results)\n",
        "            ) + \"\\nAsk about eligibility, docs, or apply.\"\n",
        "            chat_state.update({\"scheme_id\": results[0][\"scheme_id\"], \"stage\": 4})\n",
        "\n",
        "    elif chat_state[\"stage\"] == 4:\n",
        "        response = fetch_scheme_field_llm(chat_state[\"scheme_id\"], msg)\n",
        "\n",
        "    else:\n",
        "        response = \"‚ö†Ô∏è Unexpected state. Please restart.\"\n",
        "\n",
        "    # store and return\n",
        "    chat_state[\"last_bot_msg\"] = response\n",
        "    return response\n",
        "\n",
        "# === Translate Last Scheme Response ===\n",
        "def translate_last_response():\n",
        "    # now always has something if chatbot() was called at least once\n",
        "    return translate_to_telugu(chat_state[\"last_bot_msg\"])\n",
        "\n",
        "# === PDF Q&A ===\n",
        "def query_pdf(question):\n",
        "    doc = temp_coll.find_one({\"source\": \"user_uploaded\"})\n",
        "    if not doc or \"rag_chunks\" not in doc:\n",
        "        pdf_state[\"last_pdf_msg\"] = \"‚ö†Ô∏è No PDF chunks found.\"\n",
        "        return pdf_state[\"last_pdf_msg\"]\n",
        "\n",
        "    # 1) Embed the question\n",
        "    qv = embed_model.encode(question, convert_to_tensor=True)\n",
        "\n",
        "    # 2) Score each chunk\n",
        "    scored = []\n",
        "    for c in doc[\"rag_chunks\"]:\n",
        "        if \"embedding\" in c:\n",
        "            score = util.cos_sim(qv, torch.tensor(c[\"embedding\"]).to(qv.device))[0][0].item()\n",
        "            scored.append((score, c[\"chunk_text\"]))\n",
        "\n",
        "    if not scored:\n",
        "        pdf_state[\"last_pdf_msg\"] = \"‚ö†Ô∏è No embeddings to compare.\"\n",
        "        return pdf_state[\"last_pdf_msg\"]\n",
        "\n",
        "    # 3) Pick top-3 chunks\n",
        "    top = sorted(scored, key=lambda x: x[0], reverse=True)[:3]\n",
        "    context = \"\\n---\\n\".join([t[1] for t in top])\n",
        "\n",
        "    # 4) New prompt: instruct model NOT to repeat context\n",
        "    prompt = f\"\"\"You are a knowledgeable assistant. Use the following context to answer the question **briefly**.\n",
        "**Do not** include the context in your answer‚Äîonly output the answer itself.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    # 5) Invoke LLM and extract answer\n",
        "    full = llm.invoke(prompt)\n",
        "    answer = full.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "    # 6) Save and return\n",
        "    pdf_state[\"last_pdf_msg\"] = answer\n",
        "    return answer\n",
        "\n",
        "\n",
        "def translate_pdf_response():\n",
        "    return translate_to_telugu(pdf_state[\"last_pdf_msg\"]) if pdf_state[\"last_pdf_msg\"] else \"‚ö†Ô∏è Nothing to translate.\"\n",
        "\n",
        "# === Gradio UI ===\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## ü§ñ MSME Scheme Assistant\")\n",
        "    gr.ChatInterface(fn=chatbot, title=\"üí¨ MSME Chatbot\")\n",
        "\n",
        "    tbtn=gr.Button(\"üåê Translate Last Scheme Reply\")\n",
        "    tout=gr.Textbox(label=\"üó£Ô∏è Telugu Translation\",lines=3)\n",
        "    tbtn.click(fn=translate_last_response, outputs=tout)\n",
        "\n",
        "    gr.Markdown(\"## üìÑ Chat with Uploaded PDF\")\n",
        "    pdf_btn=gr.Button(\"üìÑ Enable PDF Chat\")\n",
        "    pdf_input=gr.Textbox(label=\"Ask PDF question\",visible=False)\n",
        "    pdf_ask=gr.Button(\"Ask\",visible=False)\n",
        "    pdf_out=gr.Textbox(label=\"üìú PDF Answer\",lines=6,visible=False)\n",
        "    pdf_trans_btn=gr.Button(\"üåê Translate PDF Answer\",visible=False)\n",
        "    pdf_trans_out=gr.Textbox(label=\"üó£Ô∏è Telugu PDF Translation\",lines=3,visible=False)\n",
        "\n",
        "    def show_pdf_ui():\n",
        "        return [gr.update(visible=True)]*5\n",
        "\n",
        "    pdf_btn.click(fn=show_pdf_ui,\n",
        "                  outputs=[pdf_input,pdf_ask,pdf_out,pdf_trans_btn,pdf_trans_out])\n",
        "    pdf_ask.click(fn=query_pdf,inputs=pdf_input,outputs=pdf_out)\n",
        "    pdf_trans_btn.click(fn=translate_pdf_response,outputs=pdf_trans_out)\n",
        "\n",
        "demo.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "-oLQaxWj1R7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 787,
          "referenced_widgets": [
            "2205749e49d44ce093a0f3e9a92fc962",
            "85e30cb3c2d94bdb9376c71263439f95",
            "4146ee61a6f34d1e9cb7f644af7e2cba",
            "bb9764fdb22445c8b3b84630dde86204",
            "e8ef9dc76b0544f181c318f202a1dcfc",
            "c9c01063e63543db9d5f00e483ab9295",
            "1ab1f976b5984d479cf11d6d8569b4d5",
            "0ed0414a799643279a37999eb6fd6a8d",
            "711441a077cd4cacaa347d9b7d83b28f",
            "c3e17d9b793040619d5bdf317d81a5d5",
            "30780d4d88bf42199b17cc116d7b010b"
          ]
        },
        "outputId": "b2195202-c331-46f4-de1d-807ddeb4cc8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2205749e49d44ce093a0f3e9a92fc962"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://1a0e9f8bfd6c5b2e36.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1a0e9f8bfd6c5b2e36.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VKY5ak_I9B2E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}